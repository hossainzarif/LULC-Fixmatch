{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c28ecbbb-9329-4d61-9f81-4142804000d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "import PIL\n",
    "import PIL.ImageOps\n",
    "import PIL.ImageEnhance\n",
    "import PIL.ImageDraw\n",
    "from PIL import Image\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2dfcc16a-3e4e-4088-bee0-d12fbf46f170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9049763f-42c0-40fa-9239-ee0675efdc36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\2021\\2학기 수업\\CV\\SSL\n"
     ]
    }
   ],
   "source": [
    "# root dir\n",
    "os.chdir(\"D:/2021/2학기 수업/CV/SSL/\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c7733ce-cf38-4a0e-812d-93aa6c67a8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume = ''\n",
    "eval_steps = 2**10\n",
    "total_steps = 2**20\n",
    "batch_size = 64\n",
    "lr = 0.03\n",
    "weight_decay = 0.0005\n",
    "exp_mov_avg_decay = 0.999\n",
    "mu = 7\n",
    "lambda_u = 1\n",
    "threshold = 0.95\n",
    "\n",
    "num_class = 10\n",
    "num_labeled_data = 40\n",
    "\n",
    "cifar10_mean = (0.4914, 0.4822, 0.4465)\n",
    "cifar10_std = (0.2471, 0.2435, 0.2616)\n",
    "\n",
    "random.seed(5)\n",
    "np.random.seed(5)\n",
    "torch.manual_seed(5)\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83eb28e7-296f-4f3c-805a-48eb0a6afe8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMETER_MAX = 10\n",
    "\n",
    "def _float_parameter(v, max_v):\n",
    "    return float(v) * max_v / PARAMETER_MAX\n",
    "\n",
    "def _int_parameter(v, max_v):\n",
    "    return int(v * max_v / PARAMETER_MAX)\n",
    "\n",
    "def AutoContrast(img, **kwarg):\n",
    "    return PIL.ImageOps.autocontrast(img)\n",
    "\n",
    "def Brightness(img, v, max_v, bias = 0):\n",
    "    v = _float_parameter(v, max_v) + bias\n",
    "    return PIL.ImageEnhance.Brightness(img).enhance(v)\n",
    "\n",
    "def Color(img, v, max_v, bias=0):\n",
    "    v = _float_parameter(v, max_v) + bias\n",
    "    return PIL.ImageEnhance.Color(img).enhance(v)\n",
    "\n",
    "\n",
    "def Contrast(img, v, max_v, bias=0):\n",
    "    v = _float_parameter(v, max_v) + bias\n",
    "    return PIL.ImageEnhance.Contrast(img).enhance(v)\n",
    "\n",
    "\n",
    "def Cutout(img, v, max_v, bias=0):\n",
    "    if v == 0:\n",
    "        return img\n",
    "    v = _float_parameter(v, max_v) + bias\n",
    "    v = int(v * min(img.size))\n",
    "    return CutoutAbs(img, v)\n",
    "\n",
    "\n",
    "def CutoutAbs(img, v, **kwarg):\n",
    "    w, h = img.size\n",
    "    x0 = np.random.uniform(0, w)\n",
    "    y0 = np.random.uniform(0, h)\n",
    "    x0 = int(max(0, x0 - v / 2.))\n",
    "    y0 = int(max(0, y0 - v / 2.))\n",
    "    x1 = int(min(w, x0 + v))\n",
    "    y1 = int(min(h, y0 + v))\n",
    "    xy = (x0, y0, x1, y1)\n",
    "    # gray\n",
    "    color = (127, 127, 127)\n",
    "    img = img.copy()\n",
    "    PIL.ImageDraw.Draw(img).rectangle(xy, color)\n",
    "    return img\n",
    "\n",
    "\n",
    "def Equalize(img, **kwarg):\n",
    "    return PIL.ImageOps.equalize(img)\n",
    "\n",
    "\n",
    "def Identity(img, **kwarg):\n",
    "    return img\n",
    "\n",
    "\n",
    "def Invert(img, **kwarg):\n",
    "    return PIL.ImageOps.invert(img)\n",
    "\n",
    "\n",
    "def Posterize(img, v, max_v, bias=0):\n",
    "    v = _int_parameter(v, max_v) + bias\n",
    "    return PIL.ImageOps.posterize(img, v)\n",
    "\n",
    "\n",
    "def Rotate(img, v, max_v, bias=0):\n",
    "    v = _int_parameter(v, max_v) + bias\n",
    "    if random.random() < 0.5:\n",
    "        v = -v\n",
    "    return img.rotate(v)\n",
    "\n",
    "\n",
    "def Sharpness(img, v, max_v, bias=0):\n",
    "    v = _float_parameter(v, max_v) + bias\n",
    "    return PIL.ImageEnhance.Sharpness(img).enhance(v)\n",
    "\n",
    "\n",
    "def ShearX(img, v, max_v, bias=0):\n",
    "    v = _float_parameter(v, max_v) + bias\n",
    "    if random.random() < 0.5:\n",
    "        v = -v\n",
    "    return img.transform(img.size, PIL.Image.AFFINE, (1, v, 0, 0, 1, 0))\n",
    "\n",
    "\n",
    "def ShearY(img, v, max_v, bias=0):\n",
    "    v = _float_parameter(v, max_v) + bias\n",
    "    if random.random() < 0.5:\n",
    "        v = -v\n",
    "    return img.transform(img.size, PIL.Image.AFFINE, (1, 0, 0, v, 1, 0))\n",
    "\n",
    "\n",
    "def Solarize(img, v, max_v, bias=0):\n",
    "    v = _int_parameter(v, max_v) + bias\n",
    "    return PIL.ImageOps.solarize(img, 256 - v)\n",
    "\n",
    "\n",
    "def SolarizeAdd(img, v, max_v, bias=0, threshold=128):\n",
    "    v = _int_parameter(v, max_v) + bias\n",
    "    if random.random() < 0.5:\n",
    "        v = -v\n",
    "    img_np = np.array(img).astype(np.int)\n",
    "    img_np = img_np + v\n",
    "    img_np = np.clip(img_np, 0, 255)\n",
    "    img_np = img_np.astype(np.uint8)\n",
    "    img = Image.fromarray(img_np)\n",
    "    return PIL.ImageOps.solarize(img, threshold)\n",
    "\n",
    "\n",
    "def TranslateX(img, v, max_v, bias=0):\n",
    "    v = _float_parameter(v, max_v) + bias\n",
    "    if random.random() < 0.5:\n",
    "        v = -v\n",
    "    v = int(v * img.size[0])\n",
    "    return img.transform(img.size, PIL.Image.AFFINE, (1, 0, v, 0, 1, 0))\n",
    "\n",
    "\n",
    "def TranslateY(img, v, max_v, bias=0):\n",
    "    v = _float_parameter(v, max_v) + bias\n",
    "    if random.random() < 0.5:\n",
    "        v = -v\n",
    "    v = int(v * img.size[1])\n",
    "    return img.transform(img.size, PIL.Image.AFFINE, (1, 0, 0, 0, 1, v))\n",
    "\n",
    "def fixMatchAugPool():\n",
    "    augs = [(AutoContrast, None, None),\n",
    "            (Brightness, 0.9, 0.05),\n",
    "            (Color, 0.9, 0.05),\n",
    "            (Contrast, 0.9, 0.05),\n",
    "            (Equalize, None, None),\n",
    "            (Identity, None, None),\n",
    "            (Posterize, 4, 4),\n",
    "            (Rotate, 30, 0),\n",
    "            (Sharpness, 0.9, 0.05),\n",
    "            (ShearX, 0.3, 0),\n",
    "            (ShearY, 0.3, 0),\n",
    "            (Solarize, 256, 0),\n",
    "            (TranslateX, 0.3, 0),\n",
    "            (TranslateY, 0.3, 0)]\n",
    "    return augs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fde86048-35d6-4f6a-a25a-a764fb0e8511",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandAugmentMC(object):\n",
    "    def __init__(self, n, m):\n",
    "        assert n >= 1\n",
    "        assert 1 <= m <= 10\n",
    "        self.n = n\n",
    "        self.m = m\n",
    "        self.augment_pool = fixMatchAugPool()\n",
    "        \n",
    "    def __call__(self, img):\n",
    "        ops = random.choices(self.augment_pool, k = self.n)\n",
    "        for op, max_v, bias in ops:\n",
    "            v = np.random.randint(1, self.m)\n",
    "            if random.random() < 0.5:\n",
    "                img = op(img, v = v, max_v = max_v, bias = bias)\n",
    "        img = CutoutAbs(img, int(32*0.5))\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0549d814-7995-483b-8085-b400148b279d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformFixMatch(object):\n",
    "    def __init__(self, mean, std):\n",
    "        self.weak = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomCrop(size = 32,\n",
    "                                  padding = int(32 * 0.125),\n",
    "                                  padding_mode = 'reflect')])\n",
    "        \n",
    "        self.strong = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomCrop(size = 32,\n",
    "                                  padding = int(32 * 0.125),\n",
    "                                  padding_mode = 'reflect'),\n",
    "            RandAugmentMC(n = 2, m = 10)])\n",
    "        \n",
    "        self.normalize = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean = mean, std = std)])\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        weak = self.weak(x)\n",
    "        strong = self.strong(x)\n",
    "        return self.normalize(weak), self.normalize(strong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53da93ee-9d1f-4682-a485-4b96c75650fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = './data'\n",
    "labeled_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(size = 32, padding = int(32 * 0.125), padding_mode = 'reflect'),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean = cifar10_mean, std = cifar10_std)\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean = cifar10_mean, std = cifar10_std)\n",
    "])\n",
    "\n",
    "full_dataset = datasets.CIFAR10(root, train = True, download = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "554fbdc9-2566-443f-b42f-c82365065eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "65560 50000\n"
     ]
    }
   ],
   "source": [
    "label_per_class = num_labeled_data // num_class\n",
    "labels = np.array(full_dataset.targets)\n",
    "train_labeled_idxs = []\n",
    "train_unlabeled_idxs = np.array(range(len(labels)))\n",
    "for i in range(num_class):\n",
    "    idx = np.where(labels == i)[0]\n",
    "    idx = np.random.choice(idx, label_per_class, False)\n",
    "    train_labeled_idxs.extend(idx)\n",
    "train_labeled_idxs = np.array(train_labeled_idxs)\n",
    "print(len(train_labeled_idxs))\n",
    "\n",
    "num_expand_x = math.ceil(batch_size * eval_steps / num_labeled_data)\n",
    "train_labeled_idxs = np.hstack([train_labeled_idxs for _ in range(num_expand_x)])\n",
    "print(len(train_labeled_idxs), len(train_unlabeled_idxs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a9f73b6-89d9-4ae4-af53-3e4662809eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cifar10SSL(datasets.CIFAR10):\n",
    "    def __init__(self, root, idxs, train = True, transform = None, target_transform = None, download = False):\n",
    "        super().__init__(root, train = train, transform = transform, target_transform = target_transform, download = download)\n",
    "        if idxs is not None:\n",
    "            self.data = self.data[idxs]\n",
    "            self.targets = np.array(self.targets)[idxs]\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img, target = self.data[idx], self.targets[idx]\n",
    "        img = Image.fromarray(img)\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "            \n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "            \n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "956ac0e5-a7ca-4772-9507-e4afb8d70640",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labeled_dataset = Cifar10SSL(root, train_labeled_idxs, train=True, transform=labeled_transform)\n",
    "train_unlabeled_dataset = Cifar10SSL(root, train_unlabeled_idxs, train=True,\n",
    "                                     transform=TransformFixMatch(mean = cifar10_mean, std = cifar10_std))\n",
    "test_dataset = datasets.CIFAR10(\n",
    "    root, train = False, transform = test_transform, download = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ddbc7e1-5176-4393-b63b-1ba4b6c1a29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_trainloader = DataLoader(train_labeled_dataset,\n",
    "                                 sampler = RandomSampler(train_labeled_dataset),\n",
    "                                 batch_size = batch_size,\n",
    "                                 drop_last = True)\n",
    "unlabeled_trainloader = DataLoader(train_unlabeled_dataset,\n",
    "                                 sampler = RandomSampler(train_unlabeled_dataset),\n",
    "                                 batch_size = batch_size * mu,\n",
    "                                 drop_last = True)\n",
    "test_loader = DataLoader(test_dataset,\n",
    "                         sampler = SequentialSampler(test_dataset),\n",
    "                         batch_size = batch_size)\n",
    "                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fde7583-a6ba-40d3-a49f-2729aa691c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WRNBasicBlock(torch.nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, stride, drop_rate = 0.0, activate_before_residual = False):\n",
    "        super(WRNBasicBlock, self).__init__()      \n",
    "        self.bn1 = torch.nn.BatchNorm2d(in_planes, momentum = 0.001)\n",
    "        self.relu1 = torch.nn.LeakyReLU(negative_slope = 0.1, inplace = True)\n",
    "        self.conv1 = torch.nn.Conv2d(in_planes, out_planes, kernel_size = 3, stride = stride, padding = 1, bias = False)\n",
    "        \n",
    "        self.bn2 = torch.nn.BatchNorm2d(out_planes, momentum = 0.001)\n",
    "        self.relu2 = torch.nn.LeakyReLU(negative_slope = 0.1, inplace = True)\n",
    "        self.conv2 = torch.nn.Conv2d(out_planes, out_planes, kernel_size = 3, stride = 1, padding = 1, bias = False)\n",
    "        \n",
    "        self.drop_rate = drop_rate\n",
    "        self.equalInOut = (in_planes == out_planes)\n",
    "        self.convShortcut = (not self.equalInOut) and torch.nn.Conv2d(in_planes, out_planes, kernel_size = 1, stride = stride,\n",
    "                                                                padding = 0, bias = False) or None\n",
    "        self.activate_before_residual = activate_before_residual\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if not self.equalInOut and self.activate_before_residual == True:\n",
    "            x = self.relu1(self.bn1(x))\n",
    "        else:\n",
    "            out = self.relu1(self.bn1(x))\n",
    "        out = self.relu2(self.bn2(self.conv1(out if self.equalInOut else x)))\n",
    "        \n",
    "        if self.drop_rate > 0:\n",
    "            out = F.dropout(out, p=self.drop_rate, training = self.training)\n",
    "        out = self.conv2(out)\n",
    "        \n",
    "        return torch.add(x if self.equalInOut else self.convShortcut(x), out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f531b73-ab81-4a7a-93d7-903d44103b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WRNNetworkBlock(torch.nn.Module):\n",
    "    def __init__(self, nb_layers, in_planes, out_planes, block, stride, drop_rate = 0.0, activate_before_residual = False):\n",
    "        super(WRNNetworkBlock, self).__init__()\n",
    "        self.layer = self._make_layer(\n",
    "            block, in_planes, out_planes, nb_layers, stride, drop_rate, activate_before_residual)\n",
    "        \n",
    "    def _make_layer(\n",
    "            self, block, in_planes, out_planes, nb_layers, stride, drop_rate, activate_before_residual):\n",
    "        layers = []\n",
    "        for i in range(int(nb_layers)):\n",
    "            layers.append(block(i == 0 and in_planes or out_planes, out_planes,\n",
    "                                i == 0 and stride or 1, drop_rate, activate_before_residual))\n",
    "        return torch.nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "14f79209-169f-4ea0-bb1b-dd1e0eb06793",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WRN(torch.nn.Module):\n",
    "    def __init__(self, num_classes, depth = 28, widen_factor = 2, drop_rate = 0.0):\n",
    "        super(WRN, self).__init__()\n",
    "        channels = [16, 16 * widen_factor, 32 * widen_factor, 64 * widen_factor]\n",
    "        \n",
    "        # basic block의 depth = 6, basic block 외의 depth = 4\n",
    "        # 따라서 depth - 4 의 값은 6의 배수여야 함.\n",
    "        assert((depth - 4) % 6 == 0)\n",
    "        \n",
    "        n = (depth - 4) / 6\n",
    "        block = WRNBasicBlock\n",
    "        \n",
    "        self.conv1 = torch.nn.Conv2d(3, channels[0], kernel_size = 3, stride = 1, padding = 1, bias = False)\n",
    "        \n",
    "        self.block1 = WRNNetworkBlock(\n",
    "            n, channels[0], channels[1], block, 1, drop_rate, activate_before_residual = True)\n",
    "        \n",
    "        self.block2 = WRNNetworkBlock(\n",
    "            n, channels[1], channels[2], block, 2, drop_rate)\n",
    "        \n",
    "        self.block3 = WRNNetworkBlock(\n",
    "            n, channels[2], channels[3], block, 2, drop_rate)\n",
    "        \n",
    "        self.bn = torch.nn.BatchNorm2d(channels[3], momentum = 0.001)\n",
    "        self.relu = torch.nn.LeakyReLU(negative_slope = 0.1, inplace = True)\n",
    "        self.fc = torch.nn.Linear(channels[3], num_classes)\n",
    "        self.channels = channels[3]\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, torch.nn.Conv2d):\n",
    "                torch.nn.init.kaiming_normal_(m.weight, mode = 'fan_out', nonlinearity = 'leaky_relu')\n",
    "            elif isinstance(m, torch.nn.BatchNorm2d):\n",
    "                torch.nn.init.constant_(m.weight, 1.0)\n",
    "                torch.nn.init.constant_(m.bias, 0.0)\n",
    "            elif isinstance(m, torch.nn.Linear):\n",
    "                torch.nn.init.xavier_normal_(m.weight)\n",
    "                torch.nn.init.constant_(m.bias, 0.0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.block1(out)\n",
    "        out = self.block2(out)\n",
    "        out = self.block3(out)\n",
    "        out = self.relu(self.bn(out))\n",
    "        out = F.adaptive_avg_pool2d(out, 1)\n",
    "        out = out.view(-1, self.channels)\n",
    "        return self.fc(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b06e2b9-e0fd-4deb-93cc-2fd24e41a366",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WRN(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (block1): WRNNetworkBlock(\n",
       "    (layer): Sequential(\n",
       "      (0): WRNBasicBlock(\n",
       "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (convShortcut): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (1): WRNBasicBlock(\n",
       "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (2): WRNBasicBlock(\n",
       "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (3): WRNBasicBlock(\n",
       "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (block2): WRNNetworkBlock(\n",
       "    (layer): Sequential(\n",
       "      (0): WRNBasicBlock(\n",
       "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (convShortcut): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      )\n",
       "      (1): WRNBasicBlock(\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (2): WRNBasicBlock(\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (3): WRNBasicBlock(\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (block3): WRNNetworkBlock(\n",
       "    (layer): Sequential(\n",
       "      (0): WRNBasicBlock(\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (convShortcut): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      )\n",
       "      (1): WRNBasicBlock(\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (2): WRNBasicBlock(\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (3): WRNBasicBlock(\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (bn): BatchNorm2d(128, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "  (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  (fc): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = WRN(num_class)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1542c61-5813-4a43-b738-3e6a3c283748",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_decay = ['bias', 'bn']\n",
    "grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(\n",
    "            nd in n for nd in no_decay)], 'weight_decay': weight_decay},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(\n",
    "            nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "optimizer = optim.SGD(grouped_parameters, lr=lr, momentum = 0.9, nesterov = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d5b0aa4f-fb1f-429b-9979-5e08678266e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCosScheduleWithWarmup(optimizer,\n",
    "                             num_warmup_steps,\n",
    "                             num_training_steps,\n",
    "                             num_cycles = 7./16.,\n",
    "                             last_epoch = -1):\n",
    "    def _lr_lambda(current_step):\n",
    "        if current_step < num_warmup_steps:\n",
    "            return float(current_step) / float(max(1, num_warmup_steps))\n",
    "        no_progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n",
    "        return max(0., math.cos(math.pi * num_cycles * no_progress))\n",
    "    \n",
    "    return LambdaLR(optimizer, _lr_lambda, last_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f13f5c5-333d-481a-8d44-039844b3a450",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = math.ceil(total_steps / eval_steps)\n",
    "scheduler = getCosScheduleWithWarmup(optimizer, 0, total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c577014d-f003-4c85-b935-a104c9600345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exponential moving avg\n",
    "class ModelEMA(object):\n",
    "    def __init__(self, model, decay):\n",
    "        self.ema = deepcopy(model)\n",
    "        self.ema.to(device)\n",
    "        self.ema.eval()\n",
    "        self.decay = decay\n",
    "        self.ema_has_module = hasattr(self.ema, 'module')\n",
    "        self.param_keys = [k for k, _ in self.ema.named_parameters()]\n",
    "        self.buffer_keys = [k for k, _ in self.ema.named_buffers()]\n",
    "        for p in self.ema.parameters():\n",
    "            p.requires_grad_(False)\n",
    "    \n",
    "    def update(self, model):\n",
    "        needs_module = hasattr(model, 'module') and not self.ema_has_module\n",
    "        with torch.no_grad():\n",
    "            msd = model.state_dict()\n",
    "            esd = self.ema.state_dict()\n",
    "            for k in self.param_keys:\n",
    "                if needs_module:\n",
    "                    j = 'module.' + k\n",
    "                else:\n",
    "                    j = k\n",
    "                model_v = msd[j].detach()\n",
    "                ema_v = esd[k]\n",
    "                esd[k].copy_(ema_v * self.decay + (1. - self.decay) * model_v)\n",
    "\n",
    "            for k in self.buffer_keys:\n",
    "                if needs_module:\n",
    "                    j = 'module.' + k\n",
    "                else:\n",
    "                    j = k\n",
    "                esd[k].copy_(msd[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7fa2db84-153f-4fd9-b893-1c84e1ce66bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ema_model = ModelEMA(model, 0.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d5fa8913-a7de-47c0-9d19-6fcb2835944a",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "\n",
    "if resume:\n",
    "    checkpoint = torch.load(resume)\n",
    "    best_acc = checkpoint['best_acc']\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    ema_model.ema.load_state_dict(checkpoint['ema_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b03db6ec-28e3-4fa2-ae23-2829f53d588e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvgMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8a99df3d-8aae-46c5-9142-9f776f258b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interleave(x, size):\n",
    "    s = list(x.shape)\n",
    "    return x.reshape([-1, size] + s[1:]).transpose(0, 1).reshape([-1] + s[1:])\n",
    "\n",
    "def deInterleave(x, size):\n",
    "    s = list(x.shape)\n",
    "    return x.reshape([size, -1] + s[1:]).transpose(0, 1).reshape([-1] + s[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c17f3f25-46c6-421d-b5b6-500b9036b546",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calAccuracy(output, target, topk = (1,)):\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "    \n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.reshape(1, -1).expand_as(pred))\n",
    "    \n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].reshape(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a5127aa4-4617-4e02-a2c0-3b80a8199b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_loader, model, epoch):\n",
    "    batch_time = AvgMeter()\n",
    "    data_time = AvgMeter()\n",
    "    losses = AvgMeter()\n",
    "    top1 = AvgMeter()\n",
    "    top5 = AvgMeter()\n",
    "    end = time.time()\n",
    "\n",
    "    test_loader = tqdm(test_loader, disable = False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
    "            data_time.update(time.time() - end)\n",
    "            model.eval()\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = F.cross_entropy(outputs, targets)\n",
    "\n",
    "            prec1, prec5 = calAccuracy(outputs, targets, topk=(1, 5))\n",
    "            losses.update(loss.item(), inputs.shape[0])\n",
    "            top1.update(prec1.item(), inputs.shape[0])\n",
    "            top5.update(prec5.item(), inputs.shape[0])\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "            test_loader.set_description(\"Test Iter: {batch:4}/{iter:4}. Data: {data:.3f}s. Batch: {bt:.3f}s. Loss: {loss:.4f}. top1: {top1:.2f}. top5: {top5:.2f}. \".format(\n",
    "                batch=batch_idx + 1,\n",
    "                iter=len(test_loader),\n",
    "                data=data_time.avg,\n",
    "                bt=batch_time.avg,\n",
    "                loss=losses.avg,\n",
    "                top1=top1.avg,\n",
    "                top5=top5.avg,\n",
    "            ))\n",
    "        \n",
    "        test_loader.close()\n",
    "\n",
    "    return losses.avg, top1.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b750fc29-a23f-4654-85a4-7ade176e0b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                         | 0/1024 [00:54<?, ?it/s]\u001b[A\n",
      "Test Iter:  157/ 157. Data: 0.012s. Batch: 0.021s. Loss: 2.9075. top1: 21.43. top5: 72.11. : 100%|█| 157/157 [00:03<00:\n",
      "  0%|                                                                                         | 0/1024 [00:03<?, ?it/s]\n",
      "\n",
      "  0%|                                                                                          | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Test Iter:    1/ 157. Data: 0.014s. Batch: 0.023s. Loss: 2.6418. top1: 21.88. top5: 81.25. :   0%| | 0/157 [00:00<?, ?i\u001b[A\n",
      "Test Iter:    2/ 157. Data: 0.014s. Batch: 0.023s. Loss: 2.7397. top1: 23.44. top5: 77.34. :   0%| | 0/157 [00:00<?, ?i\u001b[A\n",
      "Test Iter:    3/ 157. Data: 0.014s. Batch: 0.023s. Loss: 2.7790. top1: 20.83. top5: 77.60. :   0%| | 0/157 [00:00<?, ?i\u001b[A\n",
      "Test Iter:    4/ 157. Data: 0.014s. Batch: 0.023s. Loss: 2.8472. top1: 21.48. top5: 74.61. :   0%| | 0/157 [00:00<?, ?i\u001b[A\n",
      "Test Iter:    5/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.8478. top1: 22.81. top5: 73.75. :   0%| | 0/157 [00:00<?, ?i\u001b[A\n",
      "Test Iter:    5/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.8478. top1: 22.81. top5: 73.75. :   3%| | 5/157 [00:00<00:03\u001b[A\n",
      "Test Iter:    6/ 157. Data: 0.014s. Batch: 0.023s. Loss: 2.9237. top1: 21.88. top5: 72.14. :   3%| | 5/157 [00:00<00:03\u001b[A\n",
      "Test Iter:    7/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9058. top1: 20.98. top5: 72.10. :   3%| | 5/157 [00:00<00:03\u001b[A\n",
      "Test Iter:    8/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9003. top1: 20.51. top5: 72.46. :   3%| | 5/157 [00:00<00:03\u001b[A\n",
      "Test Iter:    9/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.8836. top1: 20.83. top5: 72.40. :   3%| | 5/157 [00:00<00:03\u001b[A\n",
      "Test Iter:   10/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.8625. top1: 20.78. top5: 72.50. :   3%| | 5/157 [00:00<00:03\u001b[A\n",
      "Test Iter:   10/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.8625. top1: 20.78. top5: 72.50. :   6%| | 10/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   11/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9186. top1: 20.60. top5: 72.73. :   6%| | 10/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   12/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9293. top1: 20.44. top5: 71.88. :   6%| | 10/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   13/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9567. top1: 19.83. top5: 71.15. :   6%| | 10/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   14/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9528. top1: 19.87. top5: 71.43. :   6%| | 10/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   15/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9469. top1: 20.00. top5: 71.35. :   6%| | 10/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   15/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9469. top1: 20.00. top5: 71.35. :  10%| | 15/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   16/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9451. top1: 20.21. top5: 71.29. :  10%| | 15/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   17/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9368. top1: 20.13. top5: 71.69. :  10%| | 15/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   18/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9240. top1: 20.40. top5: 72.05. :  10%| | 15/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   19/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9177. top1: 20.39. top5: 71.71. :  10%| | 15/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   20/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9252. top1: 20.39. top5: 71.41. :  10%| | 15/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   20/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9252. top1: 20.39. top5: 71.41. :  13%|▏| 20/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   21/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9179. top1: 20.31. top5: 71.50. :  13%|▏| 20/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   22/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9277. top1: 20.38. top5: 71.16. :  13%|▏| 20/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   23/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9226. top1: 20.52. top5: 70.99. :  13%|▏| 20/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   24/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9368. top1: 20.57. top5: 71.22. :  13%|▏| 20/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   25/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9442. top1: 20.50. top5: 71.12. :  13%|▏| 20/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   25/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9442. top1: 20.50. top5: 71.12. :  16%|▏| 25/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   26/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9467. top1: 20.55. top5: 71.27. :  16%|▏| 25/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   27/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9406. top1: 20.89. top5: 71.30. :  16%|▏| 25/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   28/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9295. top1: 20.98. top5: 71.65. :  16%|▏| 25/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   29/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9404. top1: 20.96. top5: 71.77. :  16%|▏| 25/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   30/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9343. top1: 21.04. top5: 71.98. :  16%|▏| 25/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   30/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9343. top1: 21.04. top5: 71.98. :  19%|▏| 30/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   31/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9311. top1: 21.02. top5: 72.28. :  19%|▏| 30/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   32/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9286. top1: 21.04. top5: 72.56. :  19%|▏| 30/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   33/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9270. top1: 20.98. top5: 72.49. :  19%|▏| 30/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   34/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9163. top1: 20.91. top5: 72.47. :  19%|▏| 30/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   35/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9180. top1: 20.80. top5: 72.46. :  19%|▏| 30/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   35/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9180. top1: 20.80. top5: 72.46. :  22%|▏| 35/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   36/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9213. top1: 20.70. top5: 72.40. :  22%|▏| 35/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   37/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9116. top1: 20.86. top5: 72.30. :  22%|▏| 35/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   38/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9063. top1: 20.89. top5: 72.12. :  22%|▏| 35/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   39/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.8953. top1: 21.03. top5: 72.16. :  22%|▏| 35/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   40/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.8948. top1: 21.02. top5: 72.11. :  22%|▏| 35/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   40/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.8948. top1: 21.02. top5: 72.11. :  25%|▎| 40/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   41/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.8975. top1: 21.11. top5: 72.29. :  25%|▎| 40/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   42/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.8993. top1: 21.21. top5: 72.25. :  25%|▎| 40/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   43/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.8903. top1: 21.29. top5: 72.57. :  25%|▎| 40/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   44/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.8943. top1: 21.38. top5: 72.30. :  25%|▎| 40/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   45/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.8970. top1: 21.25. top5: 72.40. :  25%|▎| 40/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   45/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.8970. top1: 21.25. top5: 72.40. :  29%|▎| 45/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   46/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9008. top1: 21.20. top5: 72.35. :  29%|▎| 45/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   47/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.8997. top1: 21.21. top5: 72.41. :  29%|▎| 45/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   48/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9028. top1: 21.13. top5: 72.23. :  29%|▎| 45/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   49/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9078. top1: 21.17. top5: 72.23. :  29%|▎| 45/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   50/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9024. top1: 21.12. top5: 72.25. :  29%|▎| 45/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   50/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9024. top1: 21.12. top5: 72.25. :  32%|▎| 50/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   51/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9050. top1: 21.05. top5: 72.30. :  32%|▎| 50/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   52/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9067. top1: 21.06. top5: 72.30. :  32%|▎| 50/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   53/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9042. top1: 21.02. top5: 72.38. :  32%|▎| 50/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   54/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9035. top1: 21.04. top5: 72.42. :  32%|▎| 50/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   55/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9011. top1: 20.97. top5: 72.44. :  32%|▎| 50/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   55/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9011. top1: 20.97. top5: 72.44. :  35%|▎| 55/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   56/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.8968. top1: 21.01. top5: 72.49. :  35%|▎| 55/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   57/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.8956. top1: 21.00. top5: 72.59. :  35%|▎| 55/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   58/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.8940. top1: 21.01. top5: 72.58. :  35%|▎| 55/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   59/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.8954. top1: 21.00. top5: 72.43. :  35%|▎| 55/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   60/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.8918. top1: 21.22. top5: 72.47. :  35%|▎| 55/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   60/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.8918. top1: 21.22. top5: 72.47. :  38%|▍| 60/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   61/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.8893. top1: 21.13. top5: 72.41. :  38%|▍| 60/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   62/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.8909. top1: 20.99. top5: 72.53. :  38%|▍| 60/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   63/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.8990. top1: 21.01. top5: 72.47. :  38%|▍| 60/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   64/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9026. top1: 20.95. top5: 72.34. :  38%|▍| 60/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   65/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9045. top1: 21.08. top5: 72.33. :  38%|▍| 60/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   65/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9045. top1: 21.08. top5: 72.33. :  41%|▍| 65/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   66/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9082. top1: 21.02. top5: 72.23. :  41%|▍| 65/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   67/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9095. top1: 21.04. top5: 72.22. :  41%|▍| 65/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   68/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9078. top1: 21.12. top5: 72.15. :  41%|▍| 65/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   69/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9056. top1: 21.20. top5: 72.17. :  41%|▍| 65/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   70/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9041. top1: 21.23. top5: 72.25. :  41%|▍| 65/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   70/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9041. top1: 21.23. top5: 72.25. :  45%|▍| 70/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   71/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9038. top1: 21.28. top5: 72.25. :  45%|▍| 70/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   72/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9077. top1: 21.25. top5: 72.24. :  45%|▍| 70/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   73/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9129. top1: 21.21. top5: 72.17. :  45%|▍| 70/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   74/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9137. top1: 21.20. top5: 72.11. :  45%|▍| 70/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   75/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9109. top1: 21.23. top5: 72.08. :  45%|▍| 70/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   75/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9109. top1: 21.23. top5: 72.08. :  48%|▍| 75/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   76/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9110. top1: 21.18. top5: 72.08. :  48%|▍| 75/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   77/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9101. top1: 21.29. top5: 72.10. :  48%|▍| 75/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   78/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9060. top1: 21.35. top5: 72.20. :  48%|▍| 75/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   79/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9056. top1: 21.40. top5: 72.11. :  48%|▍| 75/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   80/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9020. top1: 21.46. top5: 72.11. :  48%|▍| 75/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   80/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9020. top1: 21.46. top5: 72.11. :  51%|▌| 80/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   81/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.8976. top1: 21.64. top5: 72.18. :  51%|▌| 80/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   82/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.8994. top1: 21.61. top5: 72.18. :  51%|▌| 80/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   83/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9002. top1: 21.52. top5: 72.14. :  51%|▌| 80/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   84/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9099. top1: 21.43. top5: 71.91. :  51%|▌| 80/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   85/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9136. top1: 21.40. top5: 71.78. :  51%|▌| 80/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   85/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9136. top1: 21.40. top5: 71.78. :  54%|▌| 85/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   86/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9117. top1: 21.44. top5: 71.86. :  54%|▌| 85/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   87/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9147. top1: 21.37. top5: 71.75. :  54%|▌| 85/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   88/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9178. top1: 21.32. top5: 71.68. :  54%|▌| 85/157 [00:02<00:0\u001b[A\n",
      "Test Iter:   89/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9190. top1: 21.31. top5: 71.63. :  54%|▌| 85/157 [00:02<00:0\u001b[A\n",
      "Test Iter:   90/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9169. top1: 21.32. top5: 71.68. :  54%|▌| 85/157 [00:02<00:0\u001b[A\n",
      "Test Iter:   90/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9169. top1: 21.32. top5: 71.68. :  57%|▌| 90/157 [00:02<00:0\u001b[A\n",
      "Test Iter:   91/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9160. top1: 21.27. top5: 71.72. :  57%|▌| 90/157 [00:02<00:0\u001b[A\n",
      "Test Iter:   92/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9156. top1: 21.26. top5: 71.74. :  57%|▌| 90/157 [00:02<00:0\u001b[A\n",
      "Test Iter:   93/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9201. top1: 21.19. top5: 71.67. :  57%|▌| 90/157 [00:02<00:0\u001b[A\n",
      "Test Iter:   94/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9181. top1: 21.16. top5: 71.68. :  57%|▌| 90/157 [00:02<00:0\u001b[A\n",
      "Test Iter:   95/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9180. top1: 21.17. top5: 71.76. :  57%|▌| 90/157 [00:02<00:0\u001b[A\n",
      "Test Iter:   95/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9180. top1: 21.17. top5: 71.76. :  61%|▌| 95/157 [00:02<00:0\u001b[A\n",
      "Test Iter:   96/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9153. top1: 21.18. top5: 71.74. :  61%|▌| 95/157 [00:02<00:0\u001b[A\n",
      "Test Iter:   97/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9151. top1: 21.23. top5: 71.78. :  61%|▌| 95/157 [00:02<00:0\u001b[A\n",
      "Test Iter:   98/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9154. top1: 21.25. top5: 71.72. :  61%|▌| 95/157 [00:02<00:0\u001b[A\n",
      "Test Iter:   99/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9134. top1: 21.24. top5: 71.75. :  61%|▌| 95/157 [00:02<00:0\u001b[A\n",
      "Test Iter:  100/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9138. top1: 21.27. top5: 71.73. :  61%|▌| 95/157 [00:02<00:0\u001b[A\n",
      "Test Iter:  100/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9138. top1: 21.27. top5: 71.73. :  64%|▋| 100/157 [00:02<00:\u001b[A\n",
      "Test Iter:  101/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9159. top1: 21.26. top5: 71.72. :  64%|▋| 100/157 [00:02<00:\u001b[A\n",
      "Test Iter:  102/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9178. top1: 21.25. top5: 71.71. :  64%|▋| 100/157 [00:02<00:\u001b[A\n",
      "Test Iter:  103/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9195. top1: 21.21. top5: 71.69. :  64%|▋| 100/157 [00:02<00:\u001b[A\n",
      "Test Iter:  104/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9180. top1: 21.24. top5: 71.68. :  64%|▋| 100/157 [00:02<00:\u001b[A\n",
      "Test Iter:  105/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9164. top1: 21.24. top5: 71.74. :  64%|▋| 100/157 [00:02<00:\u001b[A\n",
      "Test Iter:  105/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9164. top1: 21.24. top5: 71.74. :  67%|▋| 105/157 [00:02<00:\u001b[A\n",
      "Test Iter:  106/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9155. top1: 21.33. top5: 71.74. :  67%|▋| 105/157 [00:02<00:\u001b[A\n",
      "Test Iter:  107/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9134. top1: 21.36. top5: 71.76. :  67%|▋| 105/157 [00:02<00:\u001b[A\n",
      "Test Iter:  108/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9088. top1: 21.47. top5: 71.85. :  67%|▋| 105/157 [00:02<00:\u001b[A\n",
      "Test Iter:  109/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9088. top1: 21.52. top5: 71.88. :  67%|▋| 105/157 [00:02<00:\u001b[A\n",
      "Test Iter:  110/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9124. top1: 21.49. top5: 71.78. :  67%|▋| 105/157 [00:02<00:\u001b[A\n",
      "Test Iter:  110/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9124. top1: 21.49. top5: 71.78. :  70%|▋| 110/157 [00:02<00:\u001b[A\n",
      "Test Iter:  111/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9133. top1: 21.44. top5: 71.72. :  70%|▋| 110/157 [00:02<00:\u001b[A\n",
      "Test Iter:  112/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9112. top1: 21.47. top5: 71.75. :  70%|▋| 110/157 [00:02<00:\u001b[A\n",
      "Test Iter:  113/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9141. top1: 21.45. top5: 71.75. :  70%|▋| 110/157 [00:02<00:\u001b[A\n",
      "Test Iter:  114/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9142. top1: 21.48. top5: 71.78. :  70%|▋| 110/157 [00:02<00:\u001b[A\n",
      "Test Iter:  115/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9137. top1: 21.47. top5: 71.85. :  70%|▋| 110/157 [00:02<00:\u001b[A\n",
      "Test Iter:  115/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9137. top1: 21.47. top5: 71.85. :  73%|▋| 115/157 [00:02<00:\u001b[A\n",
      "Test Iter:  116/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9141. top1: 21.48. top5: 71.89. :  73%|▋| 115/157 [00:02<00:\u001b[A\n",
      "Test Iter:  117/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9108. top1: 21.54. top5: 71.90. :  73%|▋| 115/157 [00:02<00:\u001b[A\n",
      "Test Iter:  118/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9106. top1: 21.52. top5: 71.89. :  73%|▋| 115/157 [00:02<00:\u001b[A\n",
      "Test Iter:  119/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9125. top1: 21.51. top5: 71.88. :  73%|▋| 115/157 [00:02<00:\u001b[A\n",
      "Test Iter:  120/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9132. top1: 21.52. top5: 71.85. :  73%|▋| 115/157 [00:02<00:\u001b[A\n",
      "Test Iter:  120/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9132. top1: 21.52. top5: 71.85. :  76%|▊| 120/157 [00:02<00:\u001b[A\n",
      "Test Iter:  121/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9117. top1: 21.59. top5: 71.86. :  76%|▊| 120/157 [00:02<00:\u001b[A\n",
      "Test Iter:  122/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9098. top1: 21.55. top5: 71.89. :  76%|▊| 120/157 [00:02<00:\u001b[A\n",
      "Test Iter:  123/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9101. top1: 21.56. top5: 71.94. :  76%|▊| 120/157 [00:02<00:\u001b[A\n",
      "Test Iter:  124/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9121. top1: 21.52. top5: 71.95. :  76%|▊| 120/157 [00:02<00:\u001b[A\n",
      "Test Iter:  125/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9121. top1: 21.56. top5: 71.92. :  76%|▊| 120/157 [00:02<00:\u001b[A\n",
      "Test Iter:  125/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9121. top1: 21.56. top5: 71.92. :  80%|▊| 125/157 [00:02<00:\u001b[A\n",
      "Test Iter:  126/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9113. top1: 21.55. top5: 71.97. :  80%|▊| 125/157 [00:02<00:\u001b[A\n",
      "Test Iter:  127/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9120. top1: 21.51. top5: 72.00. :  80%|▊| 125/157 [00:02<00:\u001b[A\n",
      "Test Iter:  128/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9112. top1: 21.47. top5: 72.05. :  80%|▊| 125/157 [00:02<00:\u001b[A\n",
      "Test Iter:  129/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9109. top1: 21.39. top5: 72.08. :  80%|▊| 125/157 [00:02<00:\u001b[A\n",
      "Test Iter:  130/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9096. top1: 21.38. top5: 72.12. :  80%|▊| 125/157 [00:02<00:\u001b[A\n",
      "Test Iter:  130/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9096. top1: 21.38. top5: 72.12. :  83%|▊| 130/157 [00:02<00:\u001b[A\n",
      "Test Iter:  131/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9113. top1: 21.35. top5: 72.10. :  83%|▊| 130/157 [00:02<00:\u001b[A\n",
      "Test Iter:  132/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9127. top1: 21.32. top5: 72.06. :  83%|▊| 130/157 [00:03<00:\u001b[A\n",
      "Test Iter:  133/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9113. top1: 21.35. top5: 72.11. :  83%|▊| 130/157 [00:03<00:\u001b[A\n",
      "Test Iter:  134/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9129. top1: 21.37. top5: 72.05. :  83%|▊| 130/157 [00:03<00:\u001b[A\n",
      "Test Iter:  135/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9136. top1: 21.35. top5: 72.04. :  83%|▊| 130/157 [00:03<00:\u001b[A\n",
      "Test Iter:  135/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9136. top1: 21.35. top5: 72.04. :  86%|▊| 135/157 [00:03<00:\u001b[A\n",
      "Test Iter:  136/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9151. top1: 21.31. top5: 72.01. :  86%|▊| 135/157 [00:03<00:\u001b[A\n",
      "Test Iter:  137/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9174. top1: 21.26. top5: 71.99. :  86%|▊| 135/157 [00:03<00:\u001b[A\n",
      "Test Iter:  138/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9153. top1: 21.31. top5: 72.02. :  86%|▊| 135/157 [00:03<00:\u001b[A\n",
      "Test Iter:  139/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9153. top1: 21.28. top5: 72.01. :  86%|▊| 135/157 [00:03<00:\u001b[A\n",
      "Test Iter:  140/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9171. top1: 21.25. top5: 71.94. :  86%|▊| 135/157 [00:03<00:\u001b[A\n",
      "Test Iter:  140/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9171. top1: 21.25. top5: 71.94. :  89%|▉| 140/157 [00:03<00:\u001b[A\n",
      "Test Iter:  141/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9153. top1: 21.28. top5: 71.96. :  89%|▉| 140/157 [00:03<00:\u001b[A\n",
      "Test Iter:  142/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9150. top1: 21.23. top5: 71.95. :  89%|▉| 140/157 [00:03<00:\u001b[A\n",
      "Test Iter:  143/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9146. top1: 21.24. top5: 71.96. :  89%|▉| 140/157 [00:03<00:\u001b[A\n",
      "Test Iter:  144/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9132. top1: 21.29. top5: 71.99. :  89%|▉| 140/157 [00:03<00:\u001b[A\n",
      "Test Iter:  145/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9137. top1: 21.28. top5: 72.00. :  89%|▉| 140/157 [00:03<00:\u001b[A\n",
      "Test Iter:  145/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9137. top1: 21.28. top5: 72.00. :  92%|▉| 145/157 [00:03<00:\u001b[A\n",
      "Test Iter:  146/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9122. top1: 21.33. top5: 72.02. :  92%|▉| 145/157 [00:03<00:\u001b[A\n",
      "Test Iter:  147/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9122. top1: 21.35. top5: 71.99. :  92%|▉| 145/157 [00:03<00:\u001b[A\n",
      "Test Iter:  148/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9140. top1: 21.33. top5: 71.98. :  92%|▉| 145/157 [00:03<00:\u001b[A\n",
      "Test Iter:  149/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9120. top1: 21.32. top5: 71.97. :  92%|▉| 145/157 [00:03<00:\u001b[A\n",
      "Test Iter:  150/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9116. top1: 21.31. top5: 71.98. :  92%|▉| 145/157 [00:03<00:\u001b[A\n",
      "Test Iter:  150/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9116. top1: 21.31. top5: 71.98. :  96%|▉| 150/157 [00:03<00:\u001b[A\n",
      "Test Iter:  151/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9086. top1: 21.40. top5: 71.99. :  96%|▉| 150/157 [00:03<00:\u001b[A\n",
      "Test Iter:  152/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9092. top1: 21.39. top5: 72.00. :  96%|▉| 150/157 [00:03<00:\u001b[A\n",
      "Test Iter:  153/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9108. top1: 21.36. top5: 71.99. :  96%|▉| 150/157 [00:03<00:\u001b[A\n",
      "Test Iter:  154/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9087. top1: 21.39. top5: 72.06. :  96%|▉| 150/157 [00:03<00:\u001b[A\n",
      "Test Iter:  155/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9091. top1: 21.37. top5: 72.06. :  96%|▉| 150/157 [00:03<00:\u001b[A\n",
      "Test Iter:  155/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9091. top1: 21.37. top5: 72.06. :  99%|▉| 155/157 [00:03<00:\u001b[A\n",
      "Test Iter:  156/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9079. top1: 21.43. top5: 72.11. :  99%|▉| 155/157 [00:03<00:\u001b[A\n",
      "Test Iter:  157/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9075. top1: 21.43. top5: 72.11. : 100%|█| 157/157 [00:03<00:\u001b[A\n",
      "\n",
      "  0%|                                                                                         | 0/1024 [00:03<?, ?it/s]\u001b[A\n",
      "Test Iter:  157/ 157. Data: 0.012s. Batch: 0.022s. Loss: 2.9075. top1: 21.43. top5: 72.11. : 100%|█| 157/157 [00:03<00:\n",
      "  0%|                                                                                         | 0/1024 [00:03<?, ?it/s]\n",
      "\n",
      "  0%|                                                                                          | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Test Iter:    1/ 157. Data: 0.013s. Batch: 0.022s. Loss: 2.6418. top1: 21.88. top5: 81.25. :   0%| | 0/157 [00:00<?, ?i\u001b[A\n",
      "Test Iter:    2/ 157. Data: 0.013s. Batch: 0.022s. Loss: 2.7397. top1: 23.44. top5: 77.34. :   0%| | 0/157 [00:00<?, ?i\u001b[A\n",
      "Test Iter:    3/ 157. Data: 0.013s. Batch: 0.022s. Loss: 2.7790. top1: 20.83. top5: 77.60. :   0%| | 0/157 [00:00<?, ?i\u001b[A\n",
      "Test Iter:    4/ 157. Data: 0.013s. Batch: 0.022s. Loss: 2.8472. top1: 21.48. top5: 74.61. :   0%| | 0/157 [00:00<?, ?i\u001b[A\n",
      "Test Iter:    5/ 157. Data: 0.013s. Batch: 0.022s. Loss: 2.8478. top1: 22.81. top5: 73.75. :   0%| | 0/157 [00:00<?, ?i\u001b[A\n",
      "Test Iter:    5/ 157. Data: 0.013s. Batch: 0.022s. Loss: 2.8478. top1: 22.81. top5: 73.75. :   3%| | 5/157 [00:00<00:03\u001b[A\n",
      "Test Iter:    6/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9237. top1: 21.88. top5: 72.14. :   3%| | 5/157 [00:00<00:03\u001b[A\n",
      "Test Iter:    7/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9058. top1: 20.98. top5: 72.10. :   3%| | 5/157 [00:00<00:03\u001b[A\n",
      "Test Iter:    8/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9003. top1: 20.51. top5: 72.46. :   3%| | 5/157 [00:00<00:03\u001b[A\n",
      "Test Iter:    9/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.8836. top1: 20.83. top5: 72.40. :   3%| | 5/157 [00:00<00:03\u001b[A\n",
      "Test Iter:   10/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.8625. top1: 20.78. top5: 72.50. :   3%| | 5/157 [00:00<00:03\u001b[A\n",
      "Test Iter:   10/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.8625. top1: 20.78. top5: 72.50. :   6%| | 10/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   11/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9186. top1: 20.60. top5: 72.73. :   6%| | 10/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   12/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9293. top1: 20.44. top5: 71.88. :   6%| | 10/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   13/ 157. Data: 0.013s. Batch: 0.022s. Loss: 2.9567. top1: 19.83. top5: 71.15. :   6%| | 10/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   14/ 157. Data: 0.013s. Batch: 0.022s. Loss: 2.9528. top1: 19.87. top5: 71.43. :   6%| | 10/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   15/ 157. Data: 0.013s. Batch: 0.022s. Loss: 2.9469. top1: 20.00. top5: 71.35. :   6%| | 10/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   15/ 157. Data: 0.013s. Batch: 0.022s. Loss: 2.9469. top1: 20.00. top5: 71.35. :  10%| | 15/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   16/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9451. top1: 20.21. top5: 71.29. :  10%| | 15/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   17/ 157. Data: 0.013s. Batch: 0.022s. Loss: 2.9368. top1: 20.13. top5: 71.69. :  10%| | 15/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   18/ 157. Data: 0.013s. Batch: 0.022s. Loss: 2.9240. top1: 20.40. top5: 72.05. :  10%| | 15/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   19/ 157. Data: 0.013s. Batch: 0.022s. Loss: 2.9177. top1: 20.39. top5: 71.71. :  10%| | 15/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   20/ 157. Data: 0.013s. Batch: 0.022s. Loss: 2.9252. top1: 20.39. top5: 71.41. :  10%| | 15/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   20/ 157. Data: 0.013s. Batch: 0.022s. Loss: 2.9252. top1: 20.39. top5: 71.41. :  13%|▏| 20/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   21/ 157. Data: 0.013s. Batch: 0.022s. Loss: 2.9179. top1: 20.31. top5: 71.50. :  13%|▏| 20/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   22/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9277. top1: 20.38. top5: 71.16. :  13%|▏| 20/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   23/ 157. Data: 0.013s. Batch: 0.022s. Loss: 2.9226. top1: 20.52. top5: 70.99. :  13%|▏| 20/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   24/ 157. Data: 0.013s. Batch: 0.022s. Loss: 2.9368. top1: 20.57. top5: 71.22. :  13%|▏| 20/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   25/ 157. Data: 0.013s. Batch: 0.022s. Loss: 2.9442. top1: 20.50. top5: 71.12. :  13%|▏| 20/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   25/ 157. Data: 0.013s. Batch: 0.022s. Loss: 2.9442. top1: 20.50. top5: 71.12. :  16%|▏| 25/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   26/ 157. Data: 0.013s. Batch: 0.022s. Loss: 2.9467. top1: 20.55. top5: 71.27. :  16%|▏| 25/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   27/ 157. Data: 0.013s. Batch: 0.022s. Loss: 2.9406. top1: 20.89. top5: 71.30. :  16%|▏| 25/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   28/ 157. Data: 0.013s. Batch: 0.022s. Loss: 2.9295. top1: 20.98. top5: 71.65. :  16%|▏| 25/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   29/ 157. Data: 0.013s. Batch: 0.022s. Loss: 2.9404. top1: 20.96. top5: 71.77. :  16%|▏| 25/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   30/ 157. Data: 0.013s. Batch: 0.022s. Loss: 2.9343. top1: 21.04. top5: 71.98. :  16%|▏| 25/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   30/ 157. Data: 0.013s. Batch: 0.022s. Loss: 2.9343. top1: 21.04. top5: 71.98. :  19%|▏| 30/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   31/ 157. Data: 0.013s. Batch: 0.022s. Loss: 2.9311. top1: 21.02. top5: 72.28. :  19%|▏| 30/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   32/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9286. top1: 21.04. top5: 72.56. :  19%|▏| 30/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   33/ 157. Data: 0.013s. Batch: 0.022s. Loss: 2.9270. top1: 20.98. top5: 72.49. :  19%|▏| 30/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   34/ 157. Data: 0.013s. Batch: 0.022s. Loss: 2.9163. top1: 20.91. top5: 72.47. :  19%|▏| 30/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   35/ 157. Data: 0.013s. Batch: 0.022s. Loss: 2.9180. top1: 20.80. top5: 72.46. :  19%|▏| 30/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   35/ 157. Data: 0.013s. Batch: 0.022s. Loss: 2.9180. top1: 20.80. top5: 72.46. :  22%|▏| 35/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   36/ 157. Data: 0.013s. Batch: 0.022s. Loss: 2.9213. top1: 20.70. top5: 72.40. :  22%|▏| 35/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   37/ 157. Data: 0.013s. Batch: 0.022s. Loss: 2.9116. top1: 20.86. top5: 72.30. :  22%|▏| 35/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   38/ 157. Data: 0.013s. Batch: 0.022s. Loss: 2.9063. top1: 20.89. top5: 72.12. :  22%|▏| 35/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   39/ 157. Data: 0.013s. Batch: 0.022s. Loss: 2.8953. top1: 21.03. top5: 72.16. :  22%|▏| 35/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   40/ 157. Data: 0.013s. Batch: 0.022s. Loss: 2.8948. top1: 21.02. top5: 72.11. :  22%|▏| 35/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   40/ 157. Data: 0.013s. Batch: 0.022s. Loss: 2.8948. top1: 21.02. top5: 72.11. :  25%|▎| 40/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   41/ 157. Data: 0.013s. Batch: 0.022s. Loss: 2.8975. top1: 21.11. top5: 72.29. :  25%|▎| 40/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   42/ 157. Data: 0.013s. Batch: 0.022s. Loss: 2.8993. top1: 21.21. top5: 72.25. :  25%|▎| 40/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   43/ 157. Data: 0.013s. Batch: 0.022s. Loss: 2.8903. top1: 21.29. top5: 72.57. :  25%|▎| 40/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   44/ 157. Data: 0.013s. Batch: 0.022s. Loss: 2.8943. top1: 21.38. top5: 72.30. :  25%|▎| 40/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   45/ 157. Data: 0.013s. Batch: 0.022s. Loss: 2.8970. top1: 21.25. top5: 72.40. :  25%|▎| 40/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   45/ 157. Data: 0.013s. Batch: 0.022s. Loss: 2.8970. top1: 21.25. top5: 72.40. :  29%|▎| 45/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   46/ 157. Data: 0.013s. Batch: 0.022s. Loss: 2.9008. top1: 21.20. top5: 72.35. :  29%|▎| 45/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   47/ 157. Data: 0.013s. Batch: 0.022s. Loss: 2.8997. top1: 21.21. top5: 72.41. :  29%|▎| 45/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   48/ 157. Data: 0.013s. Batch: 0.022s. Loss: 2.9028. top1: 21.13. top5: 72.23. :  29%|▎| 45/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   49/ 157. Data: 0.013s. Batch: 0.022s. Loss: 2.9078. top1: 21.17. top5: 72.23. :  29%|▎| 45/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   50/ 157. Data: 0.013s. Batch: 0.022s. Loss: 2.9024. top1: 21.12. top5: 72.25. :  29%|▎| 45/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   50/ 157. Data: 0.013s. Batch: 0.022s. Loss: 2.9024. top1: 21.12. top5: 72.25. :  32%|▎| 50/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   51/ 157. Data: 0.013s. Batch: 0.022s. Loss: 2.9050. top1: 21.05. top5: 72.30. :  32%|▎| 50/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   52/ 157. Data: 0.013s. Batch: 0.022s. Loss: 2.9067. top1: 21.06. top5: 72.30. :  32%|▎| 50/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   53/ 157. Data: 0.013s. Batch: 0.022s. Loss: 2.9042. top1: 21.02. top5: 72.38. :  32%|▎| 50/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   54/ 157. Data: 0.013s. Batch: 0.022s. Loss: 2.9035. top1: 21.04. top5: 72.42. :  32%|▎| 50/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   55/ 157. Data: 0.013s. Batch: 0.022s. Loss: 2.9011. top1: 20.97. top5: 72.44. :  32%|▎| 50/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   55/ 157. Data: 0.013s. Batch: 0.022s. Loss: 2.9011. top1: 20.97. top5: 72.44. :  35%|▎| 55/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   56/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.8968. top1: 21.01. top5: 72.49. :  35%|▎| 55/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   57/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.8956. top1: 21.00. top5: 72.59. :  35%|▎| 55/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   58/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.8940. top1: 21.01. top5: 72.58. :  35%|▎| 55/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   59/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.8954. top1: 21.00. top5: 72.43. :  35%|▎| 55/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   60/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.8918. top1: 21.22. top5: 72.47. :  35%|▎| 55/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   60/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.8918. top1: 21.22. top5: 72.47. :  38%|▍| 60/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   61/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.8893. top1: 21.13. top5: 72.41. :  38%|▍| 60/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   62/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.8909. top1: 20.99. top5: 72.53. :  38%|▍| 60/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   63/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.8990. top1: 21.01. top5: 72.47. :  38%|▍| 60/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   64/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9026. top1: 20.95. top5: 72.34. :  38%|▍| 60/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   65/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9045. top1: 21.08. top5: 72.33. :  38%|▍| 60/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   65/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9045. top1: 21.08. top5: 72.33. :  41%|▍| 65/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   66/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9082. top1: 21.02. top5: 72.23. :  41%|▍| 65/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   67/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9095. top1: 21.04. top5: 72.22. :  41%|▍| 65/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   68/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9078. top1: 21.12. top5: 72.15. :  41%|▍| 65/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   69/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9056. top1: 21.20. top5: 72.17. :  41%|▍| 65/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   70/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9041. top1: 21.23. top5: 72.25. :  41%|▍| 65/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   70/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9041. top1: 21.23. top5: 72.25. :  45%|▍| 70/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   71/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9038. top1: 21.28. top5: 72.25. :  45%|▍| 70/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   72/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9077. top1: 21.25. top5: 72.24. :  45%|▍| 70/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   73/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9129. top1: 21.21. top5: 72.17. :  45%|▍| 70/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   74/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9137. top1: 21.20. top5: 72.11. :  45%|▍| 70/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   75/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9109. top1: 21.23. top5: 72.08. :  45%|▍| 70/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   75/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9109. top1: 21.23. top5: 72.08. :  48%|▍| 75/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   76/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9110. top1: 21.18. top5: 72.08. :  48%|▍| 75/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   77/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9101. top1: 21.29. top5: 72.10. :  48%|▍| 75/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   78/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9060. top1: 21.35. top5: 72.20. :  48%|▍| 75/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   79/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9056. top1: 21.40. top5: 72.11. :  48%|▍| 75/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   80/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9020. top1: 21.46. top5: 72.11. :  48%|▍| 75/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   80/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9020. top1: 21.46. top5: 72.11. :  51%|▌| 80/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   81/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.8976. top1: 21.64. top5: 72.18. :  51%|▌| 80/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   82/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.8994. top1: 21.61. top5: 72.18. :  51%|▌| 80/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   83/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9002. top1: 21.52. top5: 72.14. :  51%|▌| 80/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   84/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9099. top1: 21.43. top5: 71.91. :  51%|▌| 80/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   85/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9136. top1: 21.40. top5: 71.78. :  51%|▌| 80/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   85/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9136. top1: 21.40. top5: 71.78. :  54%|▌| 85/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   86/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9117. top1: 21.44. top5: 71.86. :  54%|▌| 85/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   87/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9147. top1: 21.37. top5: 71.75. :  54%|▌| 85/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   88/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9178. top1: 21.32. top5: 71.68. :  54%|▌| 85/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   89/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9190. top1: 21.31. top5: 71.63. :  54%|▌| 85/157 [00:02<00:0\u001b[A\n",
      "Test Iter:   90/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9169. top1: 21.32. top5: 71.68. :  54%|▌| 85/157 [00:02<00:0\u001b[A\n",
      "Test Iter:   90/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9169. top1: 21.32. top5: 71.68. :  57%|▌| 90/157 [00:02<00:0\u001b[A\n",
      "Test Iter:   91/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9160. top1: 21.27. top5: 71.72. :  57%|▌| 90/157 [00:02<00:0\u001b[A\n",
      "Test Iter:   92/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9156. top1: 21.26. top5: 71.74. :  57%|▌| 90/157 [00:02<00:0\u001b[A\n",
      "Test Iter:   93/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9201. top1: 21.19. top5: 71.67. :  57%|▌| 90/157 [00:02<00:0\u001b[A\n",
      "Test Iter:   94/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9181. top1: 21.16. top5: 71.68. :  57%|▌| 90/157 [00:02<00:0\u001b[A\n",
      "Test Iter:   95/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9180. top1: 21.17. top5: 71.76. :  57%|▌| 90/157 [00:02<00:0\u001b[A\n",
      "Test Iter:   95/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9180. top1: 21.17. top5: 71.76. :  61%|▌| 95/157 [00:02<00:0\u001b[A\n",
      "Test Iter:   96/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9153. top1: 21.18. top5: 71.74. :  61%|▌| 95/157 [00:02<00:0\u001b[A\n",
      "Test Iter:   97/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9151. top1: 21.23. top5: 71.78. :  61%|▌| 95/157 [00:02<00:0\u001b[A\n",
      "Test Iter:   98/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9154. top1: 21.25. top5: 71.72. :  61%|▌| 95/157 [00:02<00:0\u001b[A\n",
      "Test Iter:   99/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9134. top1: 21.24. top5: 71.75. :  61%|▌| 95/157 [00:02<00:0\u001b[A\n",
      "Test Iter:  100/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9138. top1: 21.27. top5: 71.73. :  61%|▌| 95/157 [00:02<00:0\u001b[A\n",
      "Test Iter:  100/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9138. top1: 21.27. top5: 71.73. :  64%|▋| 100/157 [00:02<00:\u001b[A\n",
      "Test Iter:  101/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9159. top1: 21.26. top5: 71.72. :  64%|▋| 100/157 [00:02<00:\u001b[A\n",
      "Test Iter:  102/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9178. top1: 21.25. top5: 71.71. :  64%|▋| 100/157 [00:02<00:\u001b[A\n",
      "Test Iter:  103/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9195. top1: 21.21. top5: 71.69. :  64%|▋| 100/157 [00:02<00:\u001b[A\n",
      "Test Iter:  104/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9180. top1: 21.24. top5: 71.68. :  64%|▋| 100/157 [00:02<00:\u001b[A\n",
      "Test Iter:  105/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9164. top1: 21.24. top5: 71.74. :  64%|▋| 100/157 [00:02<00:\u001b[A\n",
      "Test Iter:  105/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9164. top1: 21.24. top5: 71.74. :  67%|▋| 105/157 [00:02<00:\u001b[A\n",
      "Test Iter:  106/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9155. top1: 21.33. top5: 71.74. :  67%|▋| 105/157 [00:02<00:\u001b[A\n",
      "Test Iter:  107/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9134. top1: 21.36. top5: 71.76. :  67%|▋| 105/157 [00:02<00:\u001b[A\n",
      "Test Iter:  108/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9088. top1: 21.47. top5: 71.85. :  67%|▋| 105/157 [00:02<00:\u001b[A\n",
      "Test Iter:  109/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9088. top1: 21.52. top5: 71.88. :  67%|▋| 105/157 [00:02<00:\u001b[A\n",
      "Test Iter:  110/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9124. top1: 21.49. top5: 71.78. :  67%|▋| 105/157 [00:02<00:\u001b[A\n",
      "Test Iter:  110/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9124. top1: 21.49. top5: 71.78. :  70%|▋| 110/157 [00:02<00:\u001b[A\n",
      "Test Iter:  111/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9133. top1: 21.44. top5: 71.72. :  70%|▋| 110/157 [00:02<00:\u001b[A\n",
      "Test Iter:  112/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9112. top1: 21.47. top5: 71.75. :  70%|▋| 110/157 [00:02<00:\u001b[A\n",
      "Test Iter:  113/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9141. top1: 21.45. top5: 71.75. :  70%|▋| 110/157 [00:02<00:\u001b[A\n",
      "Test Iter:  114/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9142. top1: 21.48. top5: 71.78. :  70%|▋| 110/157 [00:02<00:\u001b[A\n",
      "Test Iter:  115/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9137. top1: 21.47. top5: 71.85. :  70%|▋| 110/157 [00:02<00:\u001b[A\n",
      "Test Iter:  115/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9137. top1: 21.47. top5: 71.85. :  73%|▋| 115/157 [00:02<00:\u001b[A\n",
      "Test Iter:  116/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9141. top1: 21.48. top5: 71.89. :  73%|▋| 115/157 [00:02<00:\u001b[A\n",
      "Test Iter:  117/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9108. top1: 21.54. top5: 71.90. :  73%|▋| 115/157 [00:02<00:\u001b[A\n",
      "Test Iter:  118/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9106. top1: 21.52. top5: 71.89. :  73%|▋| 115/157 [00:02<00:\u001b[A\n",
      "Test Iter:  119/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9125. top1: 21.51. top5: 71.88. :  73%|▋| 115/157 [00:02<00:\u001b[A\n",
      "Test Iter:  120/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9132. top1: 21.52. top5: 71.85. :  73%|▋| 115/157 [00:02<00:\u001b[A\n",
      "Test Iter:  120/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9132. top1: 21.52. top5: 71.85. :  76%|▊| 120/157 [00:02<00:\u001b[A\n",
      "Test Iter:  121/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9117. top1: 21.59. top5: 71.86. :  76%|▊| 120/157 [00:02<00:\u001b[A\n",
      "Test Iter:  122/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9098. top1: 21.55. top5: 71.89. :  76%|▊| 120/157 [00:02<00:\u001b[A\n",
      "Test Iter:  123/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9101. top1: 21.56. top5: 71.94. :  76%|▊| 120/157 [00:02<00:\u001b[A\n",
      "Test Iter:  124/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9121. top1: 21.52. top5: 71.95. :  76%|▊| 120/157 [00:02<00:\u001b[A\n",
      "Test Iter:  125/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9121. top1: 21.56. top5: 71.92. :  76%|▊| 120/157 [00:02<00:\u001b[A\n",
      "Test Iter:  125/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9121. top1: 21.56. top5: 71.92. :  80%|▊| 125/157 [00:02<00:\u001b[A\n",
      "Test Iter:  126/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9113. top1: 21.55. top5: 71.97. :  80%|▊| 125/157 [00:02<00:\u001b[A\n",
      "Test Iter:  127/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9120. top1: 21.51. top5: 72.00. :  80%|▊| 125/157 [00:02<00:\u001b[A\n",
      "Test Iter:  128/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9112. top1: 21.47. top5: 72.05. :  80%|▊| 125/157 [00:02<00:\u001b[A\n",
      "Test Iter:  129/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9109. top1: 21.39. top5: 72.08. :  80%|▊| 125/157 [00:02<00:\u001b[A\n",
      "Test Iter:  130/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9096. top1: 21.38. top5: 72.12. :  80%|▊| 125/157 [00:02<00:\u001b[A\n",
      "Test Iter:  130/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9096. top1: 21.38. top5: 72.12. :  83%|▊| 130/157 [00:02<00:\u001b[A\n",
      "Test Iter:  131/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9113. top1: 21.35. top5: 72.10. :  83%|▊| 130/157 [00:02<00:\u001b[A\n",
      "Test Iter:  132/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9127. top1: 21.32. top5: 72.06. :  83%|▊| 130/157 [00:02<00:\u001b[A\n",
      "Test Iter:  133/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9113. top1: 21.35. top5: 72.11. :  83%|▊| 130/157 [00:03<00:\u001b[A\n",
      "Test Iter:  134/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9129. top1: 21.37. top5: 72.05. :  83%|▊| 130/157 [00:03<00:\u001b[A\n",
      "Test Iter:  135/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9136. top1: 21.35. top5: 72.04. :  83%|▊| 130/157 [00:03<00:\u001b[A\n",
      "Test Iter:  135/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9136. top1: 21.35. top5: 72.04. :  86%|▊| 135/157 [00:03<00:\u001b[A\n",
      "Test Iter:  136/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9151. top1: 21.31. top5: 72.01. :  86%|▊| 135/157 [00:03<00:\u001b[A\n",
      "Test Iter:  137/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9174. top1: 21.26. top5: 71.99. :  86%|▊| 135/157 [00:03<00:\u001b[A\n",
      "Test Iter:  138/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9153. top1: 21.31. top5: 72.02. :  86%|▊| 135/157 [00:03<00:\u001b[A\n",
      "Test Iter:  139/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9153. top1: 21.28. top5: 72.01. :  86%|▊| 135/157 [00:03<00:\u001b[A\n",
      "Test Iter:  140/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9171. top1: 21.25. top5: 71.94. :  86%|▊| 135/157 [00:03<00:\u001b[A\n",
      "Test Iter:  140/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9171. top1: 21.25. top5: 71.94. :  89%|▉| 140/157 [00:03<00:\u001b[A\n",
      "Test Iter:  141/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9153. top1: 21.28. top5: 71.96. :  89%|▉| 140/157 [00:03<00:\u001b[A\n",
      "Test Iter:  142/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9150. top1: 21.23. top5: 71.95. :  89%|▉| 140/157 [00:03<00:\u001b[A\n",
      "Test Iter:  143/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9146. top1: 21.24. top5: 71.96. :  89%|▉| 140/157 [00:03<00:\u001b[A\n",
      "Test Iter:  144/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9132. top1: 21.29. top5: 71.99. :  89%|▉| 140/157 [00:03<00:\u001b[A\n",
      "Test Iter:  145/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9137. top1: 21.28. top5: 72.00. :  89%|▉| 140/157 [00:03<00:\u001b[A\n",
      "Test Iter:  145/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9137. top1: 21.28. top5: 72.00. :  92%|▉| 145/157 [00:03<00:\u001b[A\n",
      "Test Iter:  146/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9122. top1: 21.33. top5: 72.02. :  92%|▉| 145/157 [00:03<00:\u001b[A\n",
      "Test Iter:  147/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9122. top1: 21.35. top5: 71.99. :  92%|▉| 145/157 [00:03<00:\u001b[A\n",
      "Test Iter:  148/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9140. top1: 21.33. top5: 71.98. :  92%|▉| 145/157 [00:03<00:\u001b[A\n",
      "Test Iter:  149/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9120. top1: 21.32. top5: 71.97. :  92%|▉| 145/157 [00:03<00:\u001b[A\n",
      "Test Iter:  150/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9116. top1: 21.31. top5: 71.98. :  92%|▉| 145/157 [00:03<00:\u001b[A\n",
      "Test Iter:  150/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9116. top1: 21.31. top5: 71.98. :  96%|▉| 150/157 [00:03<00:\u001b[A\n",
      "Test Iter:  151/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9086. top1: 21.40. top5: 71.99. :  96%|▉| 150/157 [00:03<00:\u001b[A\n",
      "Test Iter:  152/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9092. top1: 21.39. top5: 72.00. :  96%|▉| 150/157 [00:03<00:\u001b[A\n",
      "Test Iter:  153/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9108. top1: 21.36. top5: 71.99. :  96%|▉| 150/157 [00:03<00:\u001b[A\n",
      "Test Iter:  154/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9087. top1: 21.39. top5: 72.06. :  96%|▉| 150/157 [00:03<00:\u001b[A\n",
      "Test Iter:  155/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9091. top1: 21.37. top5: 72.06. :  96%|▉| 150/157 [00:03<00:\u001b[A\n",
      "Test Iter:  155/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9091. top1: 21.37. top5: 72.06. :  99%|▉| 155/157 [00:03<00:\u001b[A\n",
      "Test Iter:  156/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9079. top1: 21.43. top5: 72.11. :  99%|▉| 155/157 [00:03<00:\u001b[A\n",
      "Test Iter:  157/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9075. top1: 21.43. top5: 72.11. : 100%|█| 157/157 [00:03<00:\u001b[A\n",
      "\n",
      "  0%|                                                                                         | 0/1024 [00:03<?, ?it/s]\u001b[A\n",
      "Test Iter:  157/ 157. Data: 0.012s. Batch: 0.021s. Loss: 2.9075. top1: 21.43. top5: 72.11. : 100%|█| 157/157 [00:03<00:\n",
      "  0%|                                                                                         | 0/1024 [00:03<?, ?it/s]\n",
      "\n",
      "  0%|                                                                                          | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "Test Iter:    1/ 157. Data: 0.013s. Batch: 0.022s. Loss: 2.6418. top1: 21.88. top5: 81.25. :   0%| | 0/157 [00:00<?, ?i\u001b[A\n",
      "Test Iter:    2/ 157. Data: 0.013s. Batch: 0.022s. Loss: 2.7397. top1: 23.44. top5: 77.34. :   0%| | 0/157 [00:00<?, ?i\u001b[A\n",
      "Test Iter:    3/ 157. Data: 0.013s. Batch: 0.022s. Loss: 2.7790. top1: 20.83. top5: 77.60. :   0%| | 0/157 [00:00<?, ?i\u001b[A\n",
      "Test Iter:    4/ 157. Data: 0.013s. Batch: 0.022s. Loss: 2.8472. top1: 21.48. top5: 74.61. :   0%| | 0/157 [00:00<?, ?i\u001b[A\n",
      "Test Iter:    5/ 157. Data: 0.013s. Batch: 0.022s. Loss: 2.8478. top1: 22.81. top5: 73.75. :   0%| | 0/157 [00:00<?, ?i\u001b[A\n",
      "Test Iter:    5/ 157. Data: 0.013s. Batch: 0.022s. Loss: 2.8478. top1: 22.81. top5: 73.75. :   3%| | 5/157 [00:00<00:03\u001b[A\n",
      "Test Iter:    6/ 157. Data: 0.013s. Batch: 0.022s. Loss: 2.9237. top1: 21.88. top5: 72.14. :   3%| | 5/157 [00:00<00:03\u001b[A\n",
      "Test Iter:    7/ 157. Data: 0.013s. Batch: 0.022s. Loss: 2.9058. top1: 20.98. top5: 72.10. :   3%| | 5/157 [00:00<00:03\u001b[A\n",
      "Test Iter:    8/ 157. Data: 0.013s. Batch: 0.022s. Loss: 2.9003. top1: 20.51. top5: 72.46. :   3%| | 5/157 [00:00<00:03\u001b[A\n",
      "Test Iter:    9/ 157. Data: 0.013s. Batch: 0.022s. Loss: 2.8836. top1: 20.83. top5: 72.40. :   3%| | 5/157 [00:00<00:03\u001b[A\n",
      "Test Iter:   10/ 157. Data: 0.013s. Batch: 0.022s. Loss: 2.8625. top1: 20.78. top5: 72.50. :   3%| | 5/157 [00:00<00:03\u001b[A\n",
      "Test Iter:   10/ 157. Data: 0.013s. Batch: 0.022s. Loss: 2.8625. top1: 20.78. top5: 72.50. :   6%| | 10/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   11/ 157. Data: 0.013s. Batch: 0.022s. Loss: 2.9186. top1: 20.60. top5: 72.73. :   6%| | 10/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   12/ 157. Data: 0.013s. Batch: 0.022s. Loss: 2.9293. top1: 20.44. top5: 71.88. :   6%| | 10/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   13/ 157. Data: 0.013s. Batch: 0.022s. Loss: 2.9567. top1: 19.83. top5: 71.15. :   6%| | 10/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   14/ 157. Data: 0.013s. Batch: 0.022s. Loss: 2.9528. top1: 19.87. top5: 71.43. :   6%| | 10/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   15/ 157. Data: 0.013s. Batch: 0.022s. Loss: 2.9469. top1: 20.00. top5: 71.35. :   6%| | 10/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   15/ 157. Data: 0.013s. Batch: 0.022s. Loss: 2.9469. top1: 20.00. top5: 71.35. :  10%| | 15/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   16/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9451. top1: 20.21. top5: 71.29. :  10%| | 15/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   17/ 157. Data: 0.013s. Batch: 0.022s. Loss: 2.9368. top1: 20.13. top5: 71.69. :  10%| | 15/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   18/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9240. top1: 20.40. top5: 72.05. :  10%| | 15/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   19/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9177. top1: 20.39. top5: 71.71. :  10%| | 15/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   20/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9252. top1: 20.39. top5: 71.41. :  10%| | 15/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   20/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9252. top1: 20.39. top5: 71.41. :  13%|▏| 20/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   21/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9179. top1: 20.31. top5: 71.50. :  13%|▏| 20/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   22/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9277. top1: 20.38. top5: 71.16. :  13%|▏| 20/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   23/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9226. top1: 20.52. top5: 70.99. :  13%|▏| 20/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   24/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9368. top1: 20.57. top5: 71.22. :  13%|▏| 20/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   25/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9442. top1: 20.50. top5: 71.12. :  13%|▏| 20/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   25/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9442. top1: 20.50. top5: 71.12. :  16%|▏| 25/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   26/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9467. top1: 20.55. top5: 71.27. :  16%|▏| 25/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   27/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9406. top1: 20.89. top5: 71.30. :  16%|▏| 25/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   28/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9295. top1: 20.98. top5: 71.65. :  16%|▏| 25/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   29/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9404. top1: 20.96. top5: 71.77. :  16%|▏| 25/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   30/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9343. top1: 21.04. top5: 71.98. :  16%|▏| 25/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   30/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9343. top1: 21.04. top5: 71.98. :  19%|▏| 30/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   31/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9311. top1: 21.02. top5: 72.28. :  19%|▏| 30/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   32/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9286. top1: 21.04. top5: 72.56. :  19%|▏| 30/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   33/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9270. top1: 20.98. top5: 72.49. :  19%|▏| 30/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   34/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9163. top1: 20.91. top5: 72.47. :  19%|▏| 30/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   35/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9180. top1: 20.80. top5: 72.46. :  19%|▏| 30/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   35/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9180. top1: 20.80. top5: 72.46. :  22%|▏| 35/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   36/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9213. top1: 20.70. top5: 72.40. :  22%|▏| 35/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   37/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9116. top1: 20.86. top5: 72.30. :  22%|▏| 35/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   38/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9063. top1: 20.89. top5: 72.12. :  22%|▏| 35/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   39/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.8953. top1: 21.03. top5: 72.16. :  22%|▏| 35/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   40/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.8948. top1: 21.02. top5: 72.11. :  22%|▏| 35/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   40/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.8948. top1: 21.02. top5: 72.11. :  25%|▎| 40/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   41/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.8975. top1: 21.11. top5: 72.29. :  25%|▎| 40/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   42/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.8993. top1: 21.21. top5: 72.25. :  25%|▎| 40/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   43/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.8903. top1: 21.29. top5: 72.57. :  25%|▎| 40/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   44/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.8943. top1: 21.38. top5: 72.30. :  25%|▎| 40/157 [00:00<00:0\u001b[A\n",
      "Test Iter:   45/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.8970. top1: 21.25. top5: 72.40. :  25%|▎| 40/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   45/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.8970. top1: 21.25. top5: 72.40. :  29%|▎| 45/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   46/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9008. top1: 21.20. top5: 72.35. :  29%|▎| 45/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   47/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.8997. top1: 21.21. top5: 72.41. :  29%|▎| 45/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   48/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9028. top1: 21.13. top5: 72.23. :  29%|▎| 45/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   49/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9078. top1: 21.17. top5: 72.23. :  29%|▎| 45/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   50/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9024. top1: 21.12. top5: 72.25. :  29%|▎| 45/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   50/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9024. top1: 21.12. top5: 72.25. :  32%|▎| 50/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   51/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9050. top1: 21.05. top5: 72.30. :  32%|▎| 50/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   52/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9067. top1: 21.06. top5: 72.30. :  32%|▎| 50/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   53/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9042. top1: 21.02. top5: 72.38. :  32%|▎| 50/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   54/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9035. top1: 21.04. top5: 72.42. :  32%|▎| 50/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   55/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9011. top1: 20.97. top5: 72.44. :  32%|▎| 50/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   55/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.9011. top1: 20.97. top5: 72.44. :  35%|▎| 55/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   56/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.8968. top1: 21.01. top5: 72.49. :  35%|▎| 55/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   57/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.8956. top1: 21.00. top5: 72.59. :  35%|▎| 55/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   58/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.8940. top1: 21.01. top5: 72.58. :  35%|▎| 55/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   59/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.8954. top1: 21.00. top5: 72.43. :  35%|▎| 55/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   60/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.8918. top1: 21.22. top5: 72.47. :  35%|▎| 55/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   60/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.8918. top1: 21.22. top5: 72.47. :  38%|▍| 60/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   61/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.8893. top1: 21.13. top5: 72.41. :  38%|▍| 60/157 [00:01<00:0\u001b[A\n",
      "Test Iter:   62/ 157. Data: 0.013s. Batch: 0.023s. Loss: 2.8909. top1: 20.99. top5: 72.53. :  39%|▍| 62/157 [00:01<00:0\u001b[A\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20912/1131317904.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[0mtest_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mema_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mema\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m     \"\"\"\n\u001b[1;32m---> 83\u001b[1;33m     \u001b[0mtest_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[0mis_best\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbest_acc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20912/2738331537.py\u001b[0m in \u001b[0;36mtest\u001b[1;34m(test_loader, model, epoch)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m             \u001b[0mdata_time\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\CV_SSL\\lib\\site-packages\\tqdm\\std.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1180\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1181\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m                 \u001b[1;31m# Update and possibly print the progressbar.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\CV_SSL\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\CV_SSL\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    559\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 561\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    562\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\CV_SSL\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\CV_SSL\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torchvision\\datasets\\cifar.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m     96\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m         \"\"\"\n\u001b[1;32m---> 98\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torchvision\\transforms\\functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdefault_float_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.zero_grad()\n",
    "global best_acc\n",
    "end = time.time()\n",
    "best_acc = 0\n",
    "\n",
    "labeled_iter = iter(labeled_trainloader)\n",
    "unlabeled_iter = iter(unlabeled_trainloader)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    batch_time = AvgMeter()\n",
    "    data_time = AvgMeter()\n",
    "    losses = AvgMeter()\n",
    "    losses_x = AvgMeter()\n",
    "    losses_u = AvgMeter()\n",
    "    mask_probs = AvgMeter()\n",
    "    \n",
    "    p_bar = tqdm(range(eval_steps), disable = False)\n",
    "    \n",
    "    for batch_idx in range(eval_steps):\n",
    "        try:\n",
    "            inputs_x, targets_x = labeled_iter.next()\n",
    "        except:            \n",
    "            labeled_iter = iter(labeled_trainloader)\n",
    "            inputs_x, targets_x = labeled_iter.next()\n",
    "\n",
    "        try:\n",
    "            (inputs_u_w, inputs_u_s), _ = unlabeled_iter.next()\n",
    "        except:\n",
    "            unlabeled_iter = iter(unlabeled_trainloader)\n",
    "            (inputs_u_w, inputs_u_s), _ = unlabeled_iter.next()\n",
    "        \n",
    "        data_time.update(time.time() - end)\n",
    "        batch_size = inputs_x.shape[0]\n",
    "        inputs = interleave(torch.cat((inputs_x, inputs_u_w, inputs_u_s)), 2*mu + 1).to(device)\n",
    "        targets_x = targets_x.to(device)\n",
    "        logits = model(inputs)\n",
    "        logits = deInterleave(logits, 2*mu + 1)\n",
    "        logits_x = logits[:batch_size]\n",
    "        logits_u_w, logits_u_s = logits[batch_size:].chunk(2)\n",
    "        del logits\n",
    "        \n",
    "        targets_x = targets_x.long()\n",
    "        Lx = F.cross_entropy(logits_x, targets_x, reduction = 'mean')\n",
    "        \n",
    "        pseudo_label = torch.softmax(logits_u_w.detach(), dim = -1)\n",
    "        max_probs, targets_u = torch.max(pseudo_label, dim = -1)\n",
    "        mask = max_probs.ge(threshold).float()\n",
    "        Lu = (F.cross_entropy(logits_u_s, targets_u, reduction='none') * mask).mean()\n",
    "        \n",
    "        loss = Lx + lambda_u * Lu\n",
    "        \n",
    "        loss.backward()\n",
    "        losses.update(loss.item())\n",
    "        losses_x.update(Lx.item())\n",
    "        losses_u.update(Lu.item())\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        ema_model.update(model)\n",
    "        model.zero_grad()\n",
    "        \n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        mask_probs.update(mask.mean().item())\n",
    "        p_bar.set_description(\"Epoch: {epoch}/{epochs:4}. Iter: {batch:4}/{iter:4}. Loss: {loss:.4f}. Loss_x: {loss_x:.4f}. Loss_u: {loss_u:.4f}. Mask: {mask:.2f}. \".format(\n",
    "            epoch=epoch + 1,\n",
    "            epochs=epochs,\n",
    "            batch=batch_idx + 1,\n",
    "            iter=eval_steps,\n",
    "            #lr=scheduler.get_last_lr()[0],\n",
    "            #data=data_time.avg,\n",
    "            #bt=batch_time.avg,\n",
    "            loss=losses.avg,\n",
    "            loss_x=losses_x.avg,\n",
    "            loss_u=losses_u.avg,\n",
    "            mask=mask_probs.avg))\n",
    "        p_bar.update()\n",
    "        \n",
    "    p_bar.close()    \n",
    "    \n",
    "    test_model = ema_model.ema\n",
    "    \n",
    "    test_loss, test_acc = test(test_loader, test_model, epoch)\n",
    "    \n",
    "    is_best = test_acc > best_acc\n",
    "    best_acc = max(test_acc, best_acc)\n",
    "    \n",
    "    model_to_save = model.module if hasattr(model, 'module') else model\n",
    "    ema_to_save = ema_model.ema.module if hasattr(ema_model.ema, 'module') else ema_model.ema\n",
    "    \n",
    "    save_checkpoint_dir = './checkpoint/checkpoint.pth.tar'\n",
    "    state = {'epoch': epoch + 1,\n",
    "             'state_dict': model_to_save.state_dict(),\n",
    "             'ema_state_dict': ema_to_save.state_dict(),\n",
    "             'acc': test_acc,\n",
    "             'best_acc': best_acc,\n",
    "             'optimizer': optimizer.state_dict(),\n",
    "             'scheduler': scheduler.state_dict(),\n",
    "            }\n",
    "    \n",
    "    torch.save(state, save_checkpoint_dir)\n",
    "    if is_best:\n",
    "        shutil.copyfile(save_checkpoint_dir, './checkpoint/model_best.pth.tar')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96eceb0c-7e20-4b16-b548-7343ae7f5242",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
